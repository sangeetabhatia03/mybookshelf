pos = unlist(lapply(0:length(perm), FUN = function(i) which(perm == i)))
pos = diff(pos)
gray.edges = which(abs(pos) > 1)
for(g in gray.edges)
{
el = c(el, g, g + 1)
cols = c(cols, "red")
}
if(perm[1] != 1){
el = c(el, 0,  1)
cols = c(cols, "red")
}
return(list(el, cols))
}
breakpoint.graph = function(perm)
{
tmp = bpg.edgelist(perm)
edge.list = matrix(tmp[[1]], ncol = 2, byrow = T) # igraph doesn't like 0 as a vertex id.
edge.cols = tmp[[2]]
bpg = graph.edgelist(edge.list + 1, directed = F)
E(bpg)$color = edge.cols
return(bpg)
}
is.balanced = function(graph)
{
edges = E(graph)
edges.cols = E(graph)$color
}
bpg = breakpoint.graph(perm)
degree(bpg)
tmp = bpg.edgelist(perm)
edge.list = matrix(tmp[[1]], ncol = 2, byrow = T) # igraph doesn't like 0 as a vertex id.
edge.list
plot(bpg)
perm = c(3, 5, 2, 6, 4, 1)
bpg.edgelist = function(perm)
{
n = max(perm)
cols = c("black", "black")
el = c(0, perm[1], perm[n], n + 1 )
for(i in 1:(n-1))
{
if(abs(perm[i] - perm[i + 1]) != 1)
{
el = c(el, perm[i], perm[i+1])
cols = c(cols, "black")
}
}
perm = c(perm, n + 1)
pos = unlist(lapply(0:length(perm), FUN = function(i) which(perm == i)))
pos = diff(pos)
gray.edges = which(abs(pos) > 1)
for(g in gray.edges)
{
el = c(el, g, g + 1)
cols = c(cols, "red")
}
if(perm[1] != 1){
el = c(el, 0,  1)
cols = c(cols, "red")
}
return(list(el, cols))
}
breakpoint.graph = function(perm)
{
tmp = bpg.edgelist(perm)
edge.list = matrix(tmp[[1]], ncol = 2, byrow = T) # igraph doesn't like 0 as a vertex id.
edge.cols = tmp[[2]]
bpg = graph.edgelist(edge.list + 1, directed = F)
E(bpg)$color = edge.cols
return(bpg)
}
is.balanced = function(graph)
{
edges = E(graph)
edges.cols = E(graph)$color
}
bpg = breakpoint.graph(perm)
degree(bpg)
plot(bpg)
perm = c(3, 11, 7, 9,5, 4, 2, 6, 1, 8, 12, 10)
# perm = c(3, 5, 2, 6, 4, 1)
bpg.edgelist = function(perm)
{
n = max(perm)
cols = c("black", "black")
el = c(0, perm[1], perm[n], n + 1 )
for(i in 1:(n-1))
{
if(abs(perm[i] - perm[i + 1]) != 1)
{
el = c(el, perm[i], perm[i+1])
cols = c(cols, "black")
}
}
perm = c(perm, n + 1)
pos = unlist(lapply(0:length(perm), FUN = function(i) which(perm == i)))
pos = diff(pos)
gray.edges = which(abs(pos) > 1)
for(g in gray.edges)
{
el = c(el, g, g + 1)
cols = c(cols, "red")
}
if(perm[1] != 1){
el = c(el, 0,  1)
cols = c(cols, "red")
}
return(list(el, cols))
}
breakpoint.graph = function(perm)
{
tmp = bpg.edgelist(perm)
edge.list = matrix(tmp[[1]], ncol = 2, byrow = T) # igraph doesn't like 0 as a vertex id.
edge.cols = tmp[[2]]
bpg = graph.edgelist(edge.list + 1, directed = F)
E(bpg)$color = edge.cols
return(bpg)
}
is.balanced = function(graph)
{
edges = E(graph)
edges.cols = E(graph)$color
}
bpg = breakpoint.graph(perm)
plot(bpg)
is.connected(bpg)
combn(1:3, 3)
?combn
install.packages('gtools')
library(gtools)
?permutations
permutations(5,5)
perms = permutations(5,5)
perms[1,]
perm = perms[1,]
plot(breakpoint.graph(perm))
plot(breakpoint.graph(perms[2,]))
install.packages("rmarkdown")
install.packages("corrplot")
install.packages("ClueR")
install.packages("Hmisc")
install.packages("knitr")
install.packages("limma")
install.packages("MSPrep")
install.packages("bioconductor")
library(bioconductor)
install.packages("preprocessCore")
install.packages("sva")
source("https://bioconductor.org/biocLite.R")
source("http://bioconductor.org/biocLite.R")
biocLite("preprocessCore")
biocLite("sva")
biocLite("pcaMethods")
biocLite("xcms")
biocLite("faahKO")
install.packages("crmn")
library(ggplot2)
install.packages("shiny")
install.packages("shinydashboard")
install.packages("markdown")
install.packages("d3heatmap")
install.packages("devtools")
install.packages("mplot")
install.packages("devtools")
library(devtools)
install_github("garthtarr/edgebundleR")
install_github("garthtarr/pairsD3")
install.packages("networkD3")
require(googleVis)
demo(WorldBank, package = "googleVis")
install.packages("magrittr")
library(magrittr)
install.packages(c("readr", "tidyr", "dplyr"))
require(readr)
original_data = read_delim("http://www.maths.usyd.edu.au/u/gartht/Brauer2008_DataSet1.tds", delim = "\t")
View(original_data)
nutrient_names <- c(G = "Glucose", L = "Leucine", P = "Phosphate", S = "Sulfate", N = "Ammonia", U = "Uracil")
cleaned_data = original_data %>%
separate(NAME,
c("name", "BP", "MF", "systematic_name", "number"),
sep = "\\|\\|") %>%
mutate_each(funs(trimws), name:systematic_name) %>%
select(-number, -GID, -YORF, -GWEIGHT)  %>%
gather(sample, expression, G0.05:U0.3) %>%
separate(sample, c("nutrient", "rate"), sep = 1, convert = TRUE) %>%
mutate(nutrient = plyr::revalue(nutrient, nutrient_names)) %>%
filter(!is.na(expression), systematic_name != "")
require(dplyr)
require(tidyr)
cleaned_data = original_data %>%
separate(NAME,
c("name", "BP", "MF", "systematic_name", "number"),
sep = "\\|\\|") %>%
mutate_each(funs(trimws), name:systematic_name) %>%
select(-number, -GID, -YORF, -GWEIGHT)  %>%
gather(sample, expression, G0.05:U0.3) %>%
separate(sample, c("nutrient", "rate"), sep = 1, convert = TRUE) %>%
mutate(nutrient = plyr::revalue(nutrient, nutrient_names)) %>%
filter(!is.na(expression), systematic_name != "")
View(cleaned_data)
cleaned_data %>%
filter(BP == "leucine biosynthesis") %>%
ggplot(aes(rate, expression, color = nutrient)) +
geom_line() +
facet_wrap(~name)
library(ggplot2)
cleaned_data %>%
filter(BP == "leucine biosynthesis") %>%
ggplot(aes(rate, expression, color = nutrient)) +
geom_line() +
facet_wrap(~name)
```{r simpleplot, echo=FALSE}
x <- c(1:9, 8:1)
y <- c(1, 2*(5:3), 2, -1, 17, 9, 8, 2:9)
op <- par(mfcol = c(3, 1))
for(xpd in c(FALSE, TRUE, NA)) {
plot(1:10, main = paste("xpd =", xpd))
box("figure", col = "pink", lwd = 3)
polygon(x, y, xpd = xpd, col = "orange", lty = 2, lwd = 2, border = "red")
}
par(op)
polygon(x=c(0,1,2,3), y=c(0,1,1,0))
polygon(x=c(0,1,2,3), y=c(0,1,1,0))
library(twitteR)
t1 = searchTwitter("Narendra Modi", n=500, lang = "en")
install.packages("tm")
install.packages("wordcloud")
key = "qdubd5RHKTiGimK2NA350VN3k"
secret = "WYS3hKRzja5ajFxfrKmDF3gVB8fFNtRH893571qGiGVD5ceA25"
token = "48944724-INhO0l7uzVea0KeGobyijS3xV2FCQCKfElCBxL9kn"
token_secret = "yUAc3npTjZTACkCyloSS44fVwlLqF45g6pzUGPpRvTAS4"
setup_twitter_oauth(key, secret, token,token_secret )
tweets1 = searchTwitter("Narendra Modi", n = 500, lang = "en")
tweets2 = searchTwitter("Arnab Goswami", n = 500, lang = "en")
tweets = c(tweets1, tweets2)  # Put the three searches together
library(tm)
install.packages("slam")
install.packages("tm")
library(tm)
tweets1 = searchTwitter("Narendra Modi", n = 500, lang = "en")
tweets2 = searchTwitter("Arnab Goswami", n = 500, lang = "en")
tweets = c(tweets1, tweets2)  # Put the three searches together
tweets1 = searchTwitter("Narendra Modi", n = 500, lang = "en")
library(twitteR)
install.packages("twitteR")
library(twitteR)
tweets1 = searchTwitter("Narendra Modi", n = 500, lang = "en")
key = "qdubd5RHKTiGimK2NA350VN3k"
secret = "WYS3hKRzja5ajFxfrKmDF3gVB8fFNtRH893571qGiGVD5ceA25"
token = "48944724-INhO0l7uzVea0KeGobyijS3xV2FCQCKfElCBxL9kn"
token_secret = "yUAc3npTjZTACkCyloSS44fVwlLqF45g6pzUGPpRvTAS4"
setup_twitter_oauth(key, secret, token,token_secret )
tweets1 = searchTwitter("Narendra Modi", n = 500, lang = "en")
tweets2 = searchTwitter("Arnab Goswami", n = 500, lang = "en")
tweets = c(tweets1, tweets2)  # Put the three searches together
stext = sapply(tweets, function(x) x$getText())
corpus = Corpus(VectorSource(stext)) # create a corpus
stopwords = stopwords("english")
control = list(removePunctuation = TRUE,  stopwords = stoplist,removeNumbers = TRUE, tolower = TRUE, stemming=FALSE))
tdm = TermDocumentMatrix(corpus,
tdm = TermDocumentMatrix(corpus,
control = list(removePunctuation = TRUE,
stopwords = stopwords("english"),
removeNumbers = TRUE, tolower = TRUE,
stemming=FALSE)) # No stemming
)
tdm = TermDocumentMatrix(corpus, control = list(removePunctuation = TRUE,
stopwords = stopwords("english"),
removeNumbers = TRUE, tolower = TRUE,
stemming=FALSE) # No stemming
)
tdm = TermDocumentMatrix(corpus) # No stemming
corpis
corpus
tweets1 = searchTwitter("Narendra Modi", n = 500, lang = "en")
tweets2 = searchTwitter("Arnab Goswami", n = 500, lang = "en")
tweets = c(tweets1, tweets2)  # Put the three searches together
stext = sapply(tweets, function(x) x$getText())
corpus = Corpus(VectorSource(stext)) # create a corpus
tdm = TermDocumentMatrix(corpus, control = list(removePunctuation=TRUE,
stopwords = stopwords("english"),
tolower = TRUE,
stemming = FALSE
))
install.packages("SnowballC")
library(SnowballC)
tdm = TermDocumentMatrix(corpus, control = list(removePunctuation=TRUE,
stopwords = stopwords("english"),
tolower = TRUE,
stemming = FALSE
))
tweets1 = searchTwitter("Starwars", n = 500, lang = "en")
tweets2 = searchTwitter("Startrek", n = 500, lang = "en")
tweets = c(tweets1, tweets2)  # Put the three searches together
stext = sapply(tweets, function(x) x$getText())
corpus = Corpus(VectorSource(stext)) # create a corpus
tdm = TermDocumentMatrix(corpus, control = list(removePunctuation=TRUE,
stopwords = stopwords("english"),
tolower = TRUE,
stemming = FALSE
))
tweets <- iconv(tweets, to = "utf-8", sub="")
tweets1 = searchTwitter("Starwars", n = 50, lang = "en")
tweets2 = searchTwitter("Startrek", n = 50, lang = "en")
tweets = c(tweets1, tweets2)  # Put the three searches together
tweets <- iconv(tweets, to = "utf-8", sub="")
tdm = DocumentTermMatrix(corpus, control = list(removePunctuation=TRUE,
stopwords = stopwords("english"),
tolower = TRUE,
stemming = FALSE
))
tweets1 = searchTwitter("Kevin Rudd", n = 50, lang = "en")
tweets2 = searchTwitter("Paulie Hanson", n = 50, lang = "en")
tweets = c(tweets1, tweets2)  # Put the three searches together
stext = sapply(tweets, function(x) x$getText())
corpus = Corpus(VectorSource(stext)) # create a corpus
tdm = DocumentTermMatrix(corpus, control = list(removePunctuation=TRUE,
stopwords = stopwords("english"),
tolower = TRUE,
stemming = FALSE
))
M = as.matrix(tdm)
freqs = rowSums(M)
install.packages("wordcloud")
wordcloud(names(freqs), freqs, random.order=FALSE, min.freq=10)
library(wordcloud)
wordcloud(names(freqs), freqs, random.order=FALSE, min.freq=10)
names(freqs)
colors = c(rep("red", length(tweets1)), rep("blue", length(tweets2)))
pca <- prcomp(t(M))
plot(pca$x[, 1], pca$x[, 2], col = colors, pch = 16)
pca
plot(pca$x[, 1], pca$x[, 2], col = colors, pch = 16)
D <- dist(t(M), method = "binary")
mds <- cmdscale(D, k = 2)
plot(mds[, 1], mds[, 2], col = colors, pch = 16)
iris
prcomp(iris)
prcomp(iris)
?prcomp
prcomp(iris[,-4])
prcomp(iris[,-5])
plot(prcomp(iris[,-5]))
plot(prcomp(iris[,-5]), type="l")
?cmdscale
x = c(1,1,2,1,4,5)
x = matrix(x, nrow = 3, byrow = TRUE)
x
centers = matrix(c(2,2,3,3), nrow = 2, byrow = TRUE)
centers
kmeans(x, centers = centers)
k = kmeans(x, centers = centers)
k$withinss
k$totss
k$betweenss
k = kmeans(x, centers = centers, iter.max = 1)
k
source('~/.active-rstudio-document')
computeDocumentScores(problem)
a = c(0,0,0,1,0.5,0,0,0,0.5,1,0,0,0,0,1,0)
tmat = matrix(a, nrow = 4, byrow = T)
p =eigen(tmat)
p$values
p$vectors
?eigen
mean(c(115,131,135,123,134))
?aov
install.packages("dae")
library(dae)
?yates.effects
count = c(4,0,3,,3,8,8,2,6,11,3,8)
count = c(4,0,3,3,8,8,2,6,11,3,8)
n=3
when = factor(c(rep("b",n),rep("a",n), rep("c",n), rep("i",n)))
?aov
when = factor(c(rep(”before”, n), rep(”after”, n), rep(”before”, n),
rep(”after”, n)), levels = c(”before”, ”after”))
when = factor(c(rep("before", n), rep("after", n), rep("before", n),
rep("after", n)), levels = c("before", "after"))
company = factor(c(rep("control", 2 * n), rep("impact", 2 * n)),
levels = c("control","impact"))
X = data.frame(when = when, company = company, count = count)
count = c(4, 0, 3, 3, 8, 8, 8, 2, 6, 11, 3, 8)
X = data.frame(when = when, company = company, count = count)
m = aov(count ~ when + company, data = X)
m
f = predict(m)
f
r = residuals(m)
r
mean(f)
library("twitteR")
st = searchTwitter("zika", n = 10000, lang = "en")
api_key = "qdubd5RHKTiGimK2NA350VN3k"
api_secret = "WYS3hKRzja5ajFxfrKmDF3gVB8fFNtRH893571qGiGVD5ceA25"
access_token = "48944724-INhO0l7uzVea0KeGobyijS3xV2FCQCKfElCBxL9kn"
access_secret = "yUAc3npTjZTACkCyloSS44fVwlLqF45g6pzUGPpRvTAS4"
#setup_twitter_oauth(api_key, api_secret)
setup_twitter_oauth(api_key, api_secret,access_token,access_secret )
st = searchTwitter("zika", n = 10000, lang = "en")
st = searchTwitter("zika", n = 5000, lang = "en")
df = twListToDF(st)
x = df$longitude
y = df$latitude
sum(!is.na(x))
install.packages("maps")
map("world")
library(maps)
map("world")
points(x, y, pch = 16, col = "red")
st = searchTwitter("zika", n = 15000, lang = "en")
df = twListToDF(st)
x = df$longitude
y = df$latitude
sum(!is.na(x))
map("world")
points(x, y, pch = 16, col = "red")
mat1 = matrix(c(0.69, 0.69, 1.09, 0, 1.09, 0.69, 1.38, 0.69, 0), byrow = T, nrow = 3)
mat1
mat2 = matrix(c(0.4.0.0.4), nrow=3)
mat2 = matrix(c(0.4.0.0.4), nrow = 3)
mat2 = matrix(c(0.4,0, 0.4), nrow = 3)
mat1 %*% mat2
0.276*0.276 + 0.436*0.276
norm(c(0.276, 0, 0.436), type = "2")
norm(c(0.276, 0, 0.276), type = "2")
library("Rphylip")
citation(package = "Rphylip")
library(dplyr)
install.packages("dplyr")
ClientID=c("A","A","B","C")
PolicyNo = c("1A","2A","3A","4A")
InceptionDate = c("10/11/2015","10/12/2015","10/01/2016","10/01/2016")
Base.premium = c(1000,2000,3000,4000)
clmcount = c(1,2,3,4)
polclm = data.frame(ClientID, PolicyNo, InceptionDate, Base.premium, clmcount)
View(polclm)
library(dplyr)
library(magrittr)
polclm %>%
group_by(ClientID, PolicyNo, InceptionDate) %>%                            # multiple group columns
summarise(max_hp = max(Base.premium), mean_mpg = mean(clmcount))
polclm %>%
group_by(ClientID) %>%                            # multiple group columns
summarise(max_hp = max(Base.premium), mean_mpg = mean(clmcount))
?aggregate
aggregate(polclm, list(cid = ClientID), colSums)
aggregate(polclm, list(cid = ClientID),sum)
aggregate(polclm[,c(1, 4,5)], list(cid = ClientID),sum)
polclm[,c(1, 4,5)]
install.packages(data.table)
install.packages("data.table")
colSums(Filter(is.numeric, polclm))
aggregate(polclm[,c(1, 4,5)], list(cid = ClientID),sum(Filter(is.numeric, polclm)))
?ddply
?plyr::ddply
?summarise
?summarise
plyr::ddply(polclm, "ClientID", summarise, colSums)
plyr::ddply(polclm, "ClientID", summarise, sum)
by(polclm[, c(4,5)], polclm$ClientID, colSums)
by(polclm[, c(4,5)], c(polclm$ClientID, polclm$PolicyNo), colSums)
polclm %>%
group_by(ClientID) %>%
Filter(is.numeric, .)
colSums
polclm %>% group_by(ClientID) %>% Filter(is.numeric, .)
polclm %>% group_by(ClientID) %>% Filter(is.numeric, .) %>% colSums
polclm %>% group_by(ClientID) %>%  colSums(Filter(is.numeric, .))
polclm %>% group_by(ClientID) %>%  sum(Filter(is.numeric, .))
colSums
polclm %>% group_by(ClientID) %>%  colSums(Filter(is.numeric, .))
polclm %>% group_by(ClientID) %>%  colSums
polclm %>% group_by(ClientID) %>%  colSums(is.numeric(.))
polclm %>% group_by(ClientID, PolicyNo, InceptionDate) %>%  colSums
polclm %>% group_by(ClientID, PolicyNo) %>% Filter(is.numeric, .) %>% colSums
sumall = function(dat)
{
dat %>% group_by(ClientID, PolicyNo) %>% Filter(is.numeric, .) %>% colSums
}
sumall = function(dat)
{
dat %>% Filter(is.numeric, .) %>% colSums
}
plyr::ddply(polclm, c("ClientID","PolicyNo","InceptionDate"), sumall)
install.packages('rJava', type='source')
install.packages("XLConnect")
library(XLConnect)
install.packages("XLConnect")
library(XLConnect)
install.packages("xlsx")
library(xlsx)
Sys.setenv(JAVA_HOME = '/Library/Java//Home')
Sys.setenv(LD_LIBRARY_PATH = '$LD_LIBRARY_PATH:$JAVA_HOME/lib')
install.packages("rJava")
library(rJava)
install.packages('rJava', type='source')
library(rJava)
devtools::install_github('yihui/xaringan')
install.packages("devtools")
devtools::install_github('yihui/xaringan')
setwd("~/GitWorkArea/mybookshelf")
books = read.csv("data/allbooks.csv")
View(books)
books = books[with(books, order("publication_year","name")),]
books = read.csv("data/allbooks.csv")
books[with(books, order("publication_year","name")),]
books[ order(books[,1], books[,2]), ]
books=books[ order(books[,1], books[,2]), ]
View(books)
write.csv(books, file = "data/allbooks.csv", row.names = F, quote = F)
